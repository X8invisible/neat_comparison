
@book{marsland_machine_2015,
	location = {Boca Raton},
	edition = {2nd ed.},
	title = {Machine learning : an algorithmic perspective},
	abstract = {Introduction. Linear Discriminants. The Multi-Layer Perceptron. Radial Basis Functions and Splines. Support Vector Machines. Learning with Trees. Decision by Committee: Ensemble Learning. Probability and Learning. Unsupervised Learning. Dimensionality Reduction. Optimization and Search. Evolutionary Learning. Reinforcement Learning. Markov Chain Monte Carlo ({MCMC}) Methods. Graphical Models. Python.},
	publisher = {Boca Raton : {CRC} Press},
	author = {Marsland, Stephen},
	year = {2015},
	keywords = {Algorithms., Electronic books, Machine learning.}
}

@online{sharma_understanding_2017,
	title = {Understanding Activation Functions in Neural Networks},
	url = {https://link.medium.com/1H2Vt6LQoZ},
	titleaddon = {Medium},
	author = {Sharma, Avinash},
	urlyear = {2019-10-21},
	year = {2017},
	langid = {english},
}

@book{thrun_learning_2012,
	title = {Learning to Learn},
	isbn = {978-1-4615-5529-2},
	abstract = {Over the past three decades or so, research on machine learning and data mining has led to a wide variety of algorithms that learn general functions from experience. As machine learning is maturing, it has begun to make the successful transition from academic research to various practical applications. Generic techniques such as decision trees and artificial neural networks, for example, are now being used in various commercial and industrial applications.  Learning to Learn is an exciting new research direction within machine learning. Similar to traditional machine-learning algorithms, the methods described in Learning to Learn induce general functions from experience. However, the book investigates algorithms that can change the way they generalize, i.e., practice the task of learning itself, and improve on it.  To illustrate the utility of learning to learn, it is worthwhile comparing machine learning with human learning. Humans encounter a continual stream of learning tasks. They do not just learn concepts or motor skills, they also learn bias, i.e., they learn how to generalize. As a result, humans are often able to generalize correctly from extremely few examples - often just a single example suffices to teach us a new thing.  A deeper understanding of computer programs that improve their ability to learn can have a large practical impact on the field of machine learning and beyond. In recent years, the field has made significant progress towards a theory of learning to learn along with practical new algorithms, some of which led to impressive results in real-world applications.  Learning to Learn provides a survey of some of the most exciting new research approaches, written by leading researchers in the field. Its objective is to investigate the utility and feasibility of computer programs that can learn how to learn, both from a practical and a theoretical point of view.},
	pagetotal = {346},
	publisher = {Springer Science \& Business Media},
	author = {Thrun, Sebastian and Pratt, Lorien},
	year = {2012},
	langid = {english},
	note = {Google-Books-{ID}: X\_jpBwAAQBAJ},
	keywords = {Computers / Information Technology, Computers / Intelligence ({AI}) \& Semantics}
}

@book{agoston_e._eiben_introduction_2015,
	location = {Berlin},
	edition = {Second edition.},
	title = {Introduction to evolutionary computing},
	isbn = {978-3-662-44874-8},
	series = {Natural computing series},
	publisher = {Springer},
	author = {{Agoston E. Eiben}},
	editora = {{J. E Smith (James E. ) author}},
	editoratype = {collaborator},
	year = {2015}
}

@book{nikhil_ketkar_deep_2017,
	location = {United States},
	title = {Deep learning with Python: A hands-on introduction},
	isbn = {978-1-4842-2766-4},
	shorttitle = {Deep learning with Python},
	publisher = {Apress},
	author = {{Nikhil Ketkar}},
	year = {2017},
	keywords = {{COMPUTERS}, Programming Languages}
}

@online{chris_nicholson_beginners_2019,
	title = {A Beginner's Guide to Backpropagation in Neural Networks},
	url = {http://skymind.ai/wiki/backpropagation},
	abstract = {A beginner's reference to Backpropagation, a key algorithm in training neural networks.},
	titleaddon = {Skymind},
	author = {{Chris Nicholson}},
	urlyear = {2019-11-02},
	year = {2019},
	langid = {english},
	file = {Snapshot:C\:\\Users\\Andrei\\Zotero\\storage\\MB5R6X9R\\backpropagation.html:text/html}
}

@book{rob_j_hyndman_forecasting:_2018,
	edition = {2nd edition},
	title = {Forecasting: Principles and Practice},
	url = {https://Otexts.com/fpp2/},
	shorttitle = {Forecasting},
	author = {{Rob J Hyndman} and Athanasopoulos, George},
	urlyear = {2019-11-04},
	year = {2018},
	file = {Snapshot:C\:\\Users\\Andrei\\Zotero\\storage\\FNJ7DUZD\\fpp2.html:text/html}
}

@book{gurney_introduction_1997,
	location = {Bristol, {PA}, {USA}},
	title = {An Introduction to Neural Networks},
	isbn = {978-1-85728-673-1},
	abstract = {From the Publisher:An Introduction to Nueral Networks will be warmly welcomed by a wide readership seeking an authoritative treatment of this key subject without an intimidating level of mathematics in the presentation.},
	publisher = {Taylor \& Francis, Inc.},
	author = {Gurney, Kevin},
	year = {1997}
}

@article{wang_three_2007,
	title = {Three fundamental misconceptions of Artificial Intelligence},
	volume = {19},
	issn = {0952-813X},
	url = {https://doi.org/10.1080/09528130601143109},
	doi = {10.1080/09528130601143109},
	pages = {249--268},
	number = {3},
	journaltitle = {Journal of Experimental \& Theoretical Artificial Intelligence},
	author = {Wang, Pei},
	urlyear = {2019-11-04},
	year = {2007},
	keywords = {Algorithmization, Axiomatization, Empirical reasoning, Formalization, Limitation of {AI}},
}

@book{krose_introduction_1993,
	title = {An introduction to Neural Networks},
	author = {Kröse, Ben and Smagt, Patrick van der},
	year = {1993},
	file = {Citeseer - Full Text PDF:C\:\\Users\\Andrei\\Zotero\\storage\\IYXY4Y2T\\Kröse et al. - 1993 - An introduction to Neural Networks.pdf:application/pdf;Citeseer - Snapshot:C\:\\Users\\Andrei\\Zotero\\storage\\H9I7DF9H\\summary.html:text/html}
}

@article{mcculloch_logical_1943,
	title = {A logical calculus of the ideas immanent in nervous activity},
	volume = {5},
	issn = {1522-9602},
	url = {https://doi.org/10.1007/BF02478259},
	doi = {10.1007/BF02478259},
	pages = {115--133},
	number = {4},
	journaltitle = {The bulletin of mathematical biophysics},
	shortjournal = {Bulletin of Mathematical Biophysics},
	author = {{McCulloch}, Warren S. and Pitts, Walter},
	urlyear = {2019-11-04},
	year = {1943},
	langid = {english},
	keywords = {Excitatory Synapse, Inhibitory Synapse, Nervous Activity, Spatial Summation, Temporal Summation}
}

@article{rosenblatt_perceptron:_1958,
	title = {The perceptron: a probabilistic model for information storage and organization in the brain.},
	volume = {65},
	pages = {386},
	number = {6},
	journaltitle = {Psychological review},
	author = {Rosenblatt, Frank},
	year = {1958}
}

@inproceedings{mitchell_explanation-based_1993,
	title = {Explanation-based neural network learning for robot control},
	pages = {287--294},
	booktitle = {Advances in neural information processing systems},
	author = {Mitchell, Tom M and Thrun, Sebastian B},
	year = {1993}
}

@article{goldberg_genetic_1988,
	title = {Genetic algorithms and machine learning},
	volume = {3},
	pages = {95--99},
	number = {2},
	journaltitle = {Machine learning},
	author = {Goldberg, David E and Holland, John H},
	year = {1988}
}

@book{nielsenneural,
	title = {Neural Networks and Deep Learning},
	url = {http://neuralnetworksanddeeplearning.com/},
	publisher = {Determination Press},
	author = {Nielsen, Michael A.},
	year = {2018},
	keywords = {ba-2018-hahnrico}
}

@article{baxt_use_1991,
	title = {Use of an artificial neural network for the diagnosis of myocardial infarction},
	volume = {115},
	pages = {843--848},
	number = {11},
	journaltitle = {Annals of internal medicine},
	author = {Baxt, William G},
	year = {1991}
}

@book{gershenson_artificial_2003,
	title = {Artificial Neural Networks for Beginners},
	abstract = {The scope of this teaching package is to make a brief induction to Artificial Neural Networks ({ANNs}) for people who have no previous knowledge of them. We first make a brief introduction to models of networks, for then describing in general terms {ANNs}. As an application, we explain the backpropagation algorithm, since it is widely used and many other algorithms are derived from it. The user should know algebra and the handling of functions and vectors. Differential calculus is recommendable, but not necessary. The contents of this package should be understood by people with high school education. It would be useful for people who are just curious about what are {ANNs}, or for people who want to become familiar with them, so when they study them more fully, they will already have clear notions of {ANNs}. Also, people who only want to apply the backpropagation algorithm without a detailed and formal explanation of it will find this material useful. This work should not be seen as "Nets for dummies", but of course it is not a treatise. Much of the formality is skipped for the sake of simplicity. Detailed explanations and demonstrations can be found in the referred readings. The included exercises complement the understanding of the theory. The on-line resources are highly recommended for extending this brief induction.},
	author = {Gershenson, Carlos},
	year = {2003},
	keywords = {Artificial neural networks, Neural networks, Algorithms, Artificial Intelligence, Back propagation, Business And Economics–Banking And Finance, Differential calculus, Dummies, Mathematical analysis, Neural and Evolutionary Computation, Vectors (mathematics)}
}

@online{wolfe_understanding_2018,
	title = {Understanding Compositional Pattern Producing Networks},
	url = {https://towardsdatascience.com/understanding-compositional-pattern-producing-networks-810f6bef1b88},
	abstract = {A comprehensive explanation of the theory behind {CPPNS}},
	titleaddon = {Medium},
	author = {Wolfe, Cameron},
	urlyear = {2019-11-10},
	year = {2018},
	langid = {english},
}

@article{stanley_compositional_2007,
	title = {Compositional pattern producing networks: A novel abstraction of development},
	volume = {8},
	issn = {1573-7632},
	url = {https://doi.org/10.1007/s10710-007-9028-8},
	doi = {10.1007/s10710-007-9028-8},
	abstract = {Natural {DNA} can encode complexity on an enormous scale. Researchers are attempting to achieve the same representational efficiency in computers by implementing developmental encodings, i.e. encodings that map the genotype to the phenotype through a process of growth from a small starting point to a mature form. A major challenge in in this effort is to find the right level of abstraction of biological development to capture its essential properties without introducing unnecessary inefficiencies. In this paper, a novel abstraction of natural development, called Compositional Pattern Producing Networks ({CPPNs}), is proposed. Unlike currently accepted abstractions such as iterative rewrite systems and cellular growth simulations, {CPPNs} map to the phenotype without local interaction, that is, each individual component of the phenotype is determined independently of every other component. Results produced with {CPPNs} through interactive evolution of two-dimensional images show that such an encoding can nevertheless produce structural motifs often attributed to more conventional developmental abstractions, suggesting that local interaction may not be essential to the desirable properties of natural encoding in the way that is usually assumed.},
	pages = {131--162},
	number = {2},
	journaltitle = {Genetic Programming and Evolvable Machines},
	author = {Stanley, Kenneth O.},
	year = {2007}
}

@online{heidenreich_neat:_2019,
	title = {{NEAT}: An Awesome Approach to {NeuroEvolution}},
	url = {http://hunterheidenreich.com/blog/neuroevolution-of-augmenting-topologies/},
	shorttitle = {{NEAT}},
	abstract = {{NeuroEvolution} can optimize and evolve neural network structure, and the {NEAT} algorithm was one of the first to show it as a viable approach!},
	author = {Heidenreich, Hunter},
	urldate = {2019-11-11},
	year = {2019},
	langid = {english},
}
@article{stanley_evolving_2002,
	title = {Evolving neural networks through augmenting topologies},
	volume = {10},
	pages = {99--127},
	number = {2},
	journaltitle = {Evolutionary computation},
	author = {Stanley, Kenneth O and Miikkulainen, Risto},
	year = {2002}
}

@article{aydinalp-koksal_comparison_2008,
	title = {Comparison of neural network, conditional demand analysis, and engineering approaches for modeling end-use energy consumption in the residential sector},
	volume = {85},
	issn = {0306-2619},
	url = {http://www.sciencedirect.com/science/article/pii/S030626190600136X},
	doi = {https://doi.org/10.1016/j.apenergy.2006.09.012},
	pages = {271 -- 296},
	number = {4},
	journaltitle = {Applied Energy},
	author = {Aydinalp-Koksal, Merih and Ugursal, V. Ismet},
	year = {2008},
	keywords = {Conditional demand analysis, Neural networks modeling, Residential energy consumption modeling}
}