@book{marsland_machine_2015,
	location = {Boca Raton},
	edition = {2nd ed.},
	title = {Machine learning : an algorithmic perspective},
	abstract = {Introduction. Linear Discriminants. The Multi-Layer Perceptron. Radial Basis Functions and Splines. Support Vector Machines. Learning with Trees. Decision by Committee: Ensemble Learning. Probability and Learning. Unsupervised Learning. Dimensionality Reduction. Optimization and Search. Evolutionary Learning. Reinforcement Learning. Markov Chain Monte Carlo ({MCMC}) Methods. Graphical Models. Python.},
	publisher = {Boca Raton : {CRC} Press},
	author = {Marsland, Stephen},
	year = {2015},
	keywords = {Algorithms., Electronic books, Machine learning.}
}

@online{sharma_understanding_2017,
	title = {Understanding Activation Functions in Neural Networks},
	url = {https://link.medium.com/1H2Vt6LQoZ},
	titleaddon = {Medium},
	author = {Sharma, Avinash},
	urlyear = {2019-10-21},
	year = {2017}
}

@book{thrun_learning_2012,
	title = {Learning to Learn},
	isbn = {978-1-4615-5529-2},
	abstract = {Over the past three decades or so, research on machine learning and data mining has led to a wide variety of algorithms that learn general functions from experience. As machine learning is maturing, it has begun to make the successful transition from academic research to various practical applications. Generic techniques such as decision trees and artificial neural networks, for example, are now being used in various commercial and industrial applications.  Learning to Learn is an exciting new research direction within machine learning. Similar to traditional machine-learning algorithms, the methods described in Learning to Learn induce general functions from experience. However, the book investigates algorithms that can change the way they generalize, i.e., practice the task of learning itself, and improve on it.  To illustrate the utility of learning to learn, it is worthwhile comparing machine learning with human learning. Humans encounter a continual stream of learning tasks. They do not just learn concepts or motor skills, they also learn bias, i.e., they learn how to generalize. As a result, humans are often able to generalize correctly from extremely few examples - often just a single example suffices to teach us a new thing.  A deeper understanding of computer programs that improve their ability to learn can have a large practical impact on the field of machine learning and beyond. In recent years, the field has made significant progress towards a theory of learning to learn along with practical new algorithms, some of which led to impressive results in real-world applications.  Learning to Learn provides a survey of some of the most exciting new research approaches, written by leading researchers in the field. Its objective is to investigate the utility and feasibility of computer programs that can learn how to learn, both from a practical and a theoretical point of view.},
	pagetotal = {346},
	publisher = {Springer Science \& Business Media},
	author = {Thrun, Sebastian and Pratt, Lorien},
	year = {2012},
	langid = {english},
	note = {Google-Books-{ID}: X\_jpBwAAQBAJ},
	keywords = {Computers / Information Technology, Computers / Intelligence ({AI}) \& Semantics}
}

@book{agoston_e._eiben_introduction_2015,
	location = {Berlin},
	edition = {Second edition.},
	title = {Introduction to evolutionary computing},
	isbn = {978-3-662-44874-8},
	series = {Natural computing series},
	publisher = {Springer},
	author = {{Agoston E. Eiben}},
	editora = {{J. E Smith (James E. ) author}},
	editoratype = {collaborator},
	year = {2015}
}

@book{nikhil_ketkar_deep_2017,
	location = {United States},
	title = {Deep learning with Python: A hands-on introduction},
	isbn = {978-1-4842-2766-4},
	shorttitle = {Deep learning with Python},
	publisher = {Apress},
	author = {{Nikhil Ketkar}},
	year = {2017},
	keywords = {{COMPUTERS}, Programming Languages}
}

@online{chris_nicholson_beginners_2019,
	title = {A Beginner's Guide to Backpropagation in Neural Networks},
	url = {http://skymind.ai/wiki/backpropagation},
	abstract = {A beginner's reference to Backpropagation, a key algorithm in training neural networks.},
	titleaddon = {Skymind},
	author = {{Chris Nicholson}},
	urlyear = {2019-11-02},
	year = {2019},
	langid = {english},
	file = {Snapshot:C\:\\Users\\Andrei\\Zotero\\storage\\MB5R6X9R\\backpropagation.html:text/html}
}

@book{rob_j_hyndman_forecasting:_2018,
	edition = {2nd edition},
	title = {Forecasting: Principles and Practice},
	url = {https://Otexts.com/fpp2/},
	shorttitle = {Forecasting},
	author = {{Rob J Hyndman} and Athanasopoulos, George},
	urlyear = {2019-11-04},
	year = {2018},
	file = {Snapshot:C\:\\Users\\Andrei\\Zotero\\storage\\FNJ7DUZD\\fpp2.html:text/html}
}

@book{gurney_introduction_1997,
	location = {Bristol, {PA}, {USA}},
	title = {An Introduction to Neural Networks},
	isbn = {978-1-85728-673-1},
	abstract = {From the Publisher:An Introduction to Nueral Networks will be warmly welcomed by a wide readership seeking an authoritative treatment of this key subject without an intimidating level of mathematics in the presentation.},
	publisher = {Taylor \& Francis, Inc.},
	author = {Gurney, Kevin},
	year = {1997}
}

@article{wang_three_2007,
	title = {Three fundamental misconceptions of Artificial Intelligence},
	volume = {19},
	issn = {0952-813X},
	url = {https://doi.org/10.1080/09528130601143109},
	doi = {10.1080/09528130601143109},
	pages = {249--268},
	number = {3},
	journaltitle = {Journal of Experimental \& Theoretical Artificial Intelligence},
	author = {Wang, Pei},
	urlyear = {2019-11-04},
	year = {2007},
	keywords = {Algorithmization, Axiomatization, Empirical reasoning, Formalization, Limitation of {AI}}
}

@book{krose_introduction_1993,
	title = {An introduction to Neural Networks},
	abstract = {Contents  Preface 9 I {FUNDAMENTALS} 11 1 Introduction 13 2 Fundamentals 15 2.1 A framework for distributed representation : : : : : : : : : : : : : : : : : : : : : 15 2.1.1 Processing units : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 15 2.1.2 Connections between units : : : : : : : : : : : : : : : : : : : : : : : : : : 16 2.1.3 Activation and output rules : : : : : : : : : : : : : : : : : : : : : : : : : : 16 2.2 Network topologies : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 17 2.3 Training of artificial neural networks : : : : : : : : : : : : : : : : : : : : : : : : : 18 2.3.1 Paradigms of learning : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 18 2.3.2 Modifying patterns of connectivity : : : : : : : : : : : : : : : : : : : : : : 18 2.4 Notation and terminology : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 18 2.4.1 Notation : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 19 2.4.2 Termino},
	author = {Kröse, Ben and Smagt, Patrick van der},
	year = {1993},
	file = {Citeseer - Full Text PDF:C\:\\Users\\Andrei\\Zotero\\storage\\IYXY4Y2T\\Kröse et al. - 1993 - An introduction to Neural Networks.pdf:application/pdf;Citeseer - Snapshot:C\:\\Users\\Andrei\\Zotero\\storage\\H9I7DF9H\\summary.html:text/html}
}

@article{mcculloch_logical_1943,
	title = {A logical calculus of the ideas immanent in nervous activity},
	volume = {5},
	issn = {1522-9602},
	url = {https://doi.org/10.1007/BF02478259},
	doi = {10.1007/BF02478259},
	abstract = {Because of the “all-or-none” character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
	pages = {115--133},
	number = {4},
	journaltitle = {The bulletin of mathematical biophysics},
	shortjournal = {Bulletin of Mathematical Biophysics},
	author = {{McCulloch}, Warren S. and Pitts, Walter},
	urlyear = {2019-11-04},
	year = {1943},
	langid = {english},
	keywords = {Excitatory Synapse, Inhibitory Synapse, Nervous Activity, Spatial Summation, Temporal Summation}
}

@misc{nielsenneural,
  added-at = {2019-01-15T22:46:49.000+0100},
  author = {Nielsen, Michael A.},
  biburl = {https://www.bibsonomy.org/bibtex/274383acee84241145ff4ffede9658206/slicside},
  interhash = {04d527cadd39f888fc3babcad3343362},
  intrahash = {74383acee84241145ff4ffede9658206},
  keywords = {ba-2018-hahnrico},
  publisher = {Determination Press},
  timestamp = {2019-01-15T22:46:49.000+0100},
  title = {Neural Networks and Deep Learning},
  type = {misc},
  url = {http://neuralnetworksanddeeplearning.com/},
  year = 2018
}

@article{rosenblatt_perceptron:_1958,
	title = {The perceptron: a probabilistic model for information storage and organization in the brain.},
	volume = {65},
	pages = {386},
	number = {6},
	journaltitle = {Psychological review},
	author = {Rosenblatt, Frank},
	year = {1958}
}

@inproceedings{mitchell_explanation-based_1993,
	title = {Explanation-based neural network learning for robot control},
	pages = {287--294},
	booktitle = {Advances in neural information processing systems},
	author = {Mitchell, Tom M and Thrun, Sebastian B},
	year = {1993}
}

@article{goldberg_genetic_1988,
	title = {Genetic algorithms and machine learning},
	volume = {3},
	pages = {95--99},
	number = {2},
	journaltitle = {Machine learning},
	author = {Goldberg, David E and Holland, John H},
	year = {1988}
}